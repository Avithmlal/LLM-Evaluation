# ğŸ¤– LLM Evaluation Framework

An automated, multi-agent framework for systematic evaluation and comparison of Large Language Models across different tasks and performance metrics.

## âœ¨ Features

- **Multi-Agent Architecture**: AutoGen-based evaluator agents for different task categories
- **Multiple LLM Support**: OpenAI, Anthropic, and custom model providers
- **Comprehensive Evaluation**: Summarization, Q&A, and reasoning tasks
- **Performance Analytics**: Accuracy, speed, cost, and ranking analysis
- **REST API**: Full-featured API for integration and automation
- **Web Dashboard**: Interactive interface for managing evaluations

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Frontend  â”‚    â”‚   FastAPI        â”‚    â”‚   Database      â”‚
â”‚   (Bootstrap)   â”‚â—„â”€â”€â–ºâ”‚   Backend        â”‚â—„â”€â”€â–ºâ”‚   (SQLite)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                       â”‚ Evaluation  â”‚
                       â”‚   Engine    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚AutoGen  â”‚         â”‚LLM Provider â”‚      â”‚Performance  â”‚
   â”‚Agents   â”‚         â”‚Factory      â”‚      â”‚Analytics    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip or conda for package management

### Installation

1. **Clone and navigate to the project**:
```bash
cd llmEval  # You're already here
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Set up environment variables** (optional):
```bash
# For real API usage
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
```

4. **Start the application**:
```bash
python main.py
```

5. **Open your browser** and go to: `http://localhost:8000`

## ğŸ“Š Demo Usage

The framework includes mock providers that work without API keys for demonstration purposes:

1. **Start Demo Evaluation**: Click the "Start Demo Evaluation" button on the homepage
2. **View Results**: Check the evaluations list for completion status
3. **Analyze Performance**: Access detailed results via the API endpoints

## ğŸ”— API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Web dashboard |
| `/docs` | GET | API documentation |
| `/api/v1/models` | GET | List available models |
| `/api/v1/test-cases` | GET | List test cases |
| `/api/v1/evaluations` | POST | Start evaluation |
| `/api/v1/evaluations/{id}/results` | GET | Get results |

## ğŸ¢ Business Value Demonstration

This POC demonstrates several key use cases:

### 1. **Model Selection Optimization**
- Compare GPT-4 vs Claude 3 vs other models
- Data-driven selection based on accuracy, speed, cost
- ROI: 20-40% cost reduction through optimal model selection

### 2. **Performance Monitoring**
- Automated evaluation pipelines
- Model drift detection
- ROI: Prevent 15-25% customer satisfaction drop

### 3. **Cost Analysis** 
- Token usage and cost tracking
- Price/performance optimization
- ROI: Immediate cost savings identification

## ğŸ§ª Test Categories

### Summarization
- News article summarization
- Technical document condensing
- Metrics: Compression ratio, key point coverage, accuracy

### Question & Answer
- Factual questions
- Domain-specific queries  
- Metrics: Accuracy, completeness, relevance

### Reasoning
- Logical puzzles
- Mathematical problems
- Metrics: Logic validity, step-by-step clarity

## ğŸ”§ Configuration

### Adding New Models

```python
# Add to database via API or directly:
new_model = LLMModel(
    name="Custom Model",
    provider="custom",
    model_id="custom-model-v1",
    cost_per_1k_tokens=0.01,
    max_tokens=4096
)
```

### Adding New Test Cases

```python
new_test = TestCase(
    name="Custom Test",
    category="reasoning",
    input_text="Test prompt...",
    expected_output="Expected response...",
    evaluation_criteria="Accuracy, clarity"
)
```

## ğŸ¯ Interview Presentation Points

### Technical Excellence
- **AutoGen Integration**: Demonstrates agentic AI capabilities
- **Scalable Architecture**: Ready for AWS Lambda deployment
- **Provider Abstraction**: Supports multiple LLM providers
- **Performance Analytics**: Comprehensive metrics and ranking

### Business Impact
- **Cost Optimization**: 20-40% potential savings
- **Risk Mitigation**: Model drift detection
- **Data-Driven Decisions**: Objective model comparison
- **Operational Efficiency**: Automated evaluation pipelines

### Production Readiness
- **Error Handling**: Comprehensive error management
- **Background Processing**: Non-blocking evaluations
- **API Design**: RESTful, well-documented
- **Database Design**: Normalized, scalable schema

## ğŸ”® Future Enhancements

- **AWS Lambda Deployment**: Serverless scaling
- **Real-time Dashboards**: Live performance monitoring  
- **Advanced Analytics**: ML-based insights
- **Multi-modal Support**: Vision and audio evaluation
- **Custom Metrics**: Domain-specific evaluation criteria

## ğŸ“ Project Structure

```
llmEval/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # AutoGen evaluator agents
â”‚   â”œâ”€â”€ api/              # FastAPI routes and models
â”‚   â”œâ”€â”€ database/         # SQLAlchemy models and config
â”‚   â”œâ”€â”€ evaluation/       # Core evaluation engine
â”‚   â””â”€â”€ llm_providers/    # LLM provider abstractions
â”œâ”€â”€ main.py               # FastAPI application entry point
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ¤ Contributing

This is a demonstration project for interview purposes. The architecture supports easy extension:

1. **New Providers**: Implement `BaseLLMProvider`
2. **New Evaluators**: Extend `EvaluatorAgent`
3. **New Metrics**: Add to `PerformanceMetrics` model
4. **New Test Types**: Create new test categories

## ğŸ“„ License

This project is created for demonstration purposes.

---

**Built with**: Python, FastAPI, SQLAlchemy, AutoGen, Bootstrap

**Demonstrates**: Agentic AI, LLM Evaluation, Data Analytics, API Design 